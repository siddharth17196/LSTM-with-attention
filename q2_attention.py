# -*- coding: utf-8 -*-
"""q2_attention.ipynb
A
utomatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1y2xTDzMZqBpUn1pJOk9v9zhRjbyA4P_2
"""

from google.colab import drive
drive.mount('/content/drive')
loc = 'drive/My Drive/dl_ass2/'

"""##IMPORTS"""

import numpy as np
import pandas as pd
from sklearn import metrics
from scipy import interp
import seaborn as sn
import matplotlib.pyplot as plt
from collections import Counter
import pickle
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
from torch.autograd import Variable
from torch.nn import functional as F

"""##PRE-PROCESS"""

# Commented out IPython magic to ensure Python compatibility.

def pad_features(content_ints, seq_length):
	features = np.zeros((len(content_ints), seq_length), dtype=int)
	for i, row in enumerate(content_ints):
		features[i, -len(row):] = np.array(row)[:seq_length]
	return features

def preprocess(name, func):
	dat = pd.read_csv(loc+name)
	dat.drop(dat.columns[[0, 1,2]], axis = 1, inplace = True) 
	dat = dat.tail(100)
	content = np.array(dat['content'])
	if func == 'train':
		labels = np.array(dat['sentiment'])
	punctuation = '!"#$%&\'()*+,-./:;<=>?[\\]^_`{|}~'
	# get rid of punctuation
	all_content = 'separator'.join(content)
	all_content = all_content.lower()
	all_text = ''.join([c for c in all_content if c not in punctuation])

	# split by new lines and spaces
	content_split = all_text.split('separator')
	all_text = ' '.join(content_split)

	# create a list of words
	words = all_text.split()

	# get rid of web address, twitter id, and digit
	new_content = []
	for cont in content_split:
		cont = cont.split()
		new_text = []
		for word in cont:
			if (word[0] != '@') & ('http' not in word) & (~word.isdigit()):
				# if word not in stop_words:
				new_text.append(word)
		new_content.append(new_text)

	counts = Counter(words)
	vocab = sorted(counts, key=counts.get, reverse=True)
	vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}
	print(vocab_to_int)
	## use the dict to tokenize each review in reviews_split
	## store the tokenized reviews in content_ints
	content_ints = []
	for cont in new_content:
		content_ints.append([vocab_to_int[word] for word in cont])

	# stats about vocabulary
	# print('Unique words: ', len((vocab_to_int))) 
	# print()

	# # print tokens in first review
	# print('Tokenized review: \n', content_ints[:1])
	# c_label = Counter(labels)

	# %matplotlib inline
	content_len = [len(x) for x in content_ints]
	pd.Series(content_len).hist()
	plt.show()
	pd.Series(content_len).describe()
	if func == 'train':
		label_dict = {'anger': 12,'boredom': 10,'empty': 0,'enthusiasm': 2,
		'fun': 7,'happiness': 9,'hate': 8,'love': 6,'neutral': 3,
		'relief': 11,'sadness': 1,'surprise': 5,'worry': 4}

		encoded_labels = []
		for sentiment in labels:
			encoded_labels.append(label_dict[sentiment])
		encoded_labels = np.asarray(encoded_labels)

		content_ints = np.asarray([content_ints[i] for i, l in enumerate(content_len) if l>0])
		encoded_labels = np.asarray([encoded_labels[i] for i, l in enumerate(content_len) if l>0])

		seq_length = 30
		features = pad_features(content_ints, seq_length=seq_length)

		## test statements 
		assert len(features)==len(content_ints), "The features should have as many rows as reviews."
		assert len(features[0])==seq_length, "Each feature row should contain seq_length values."

		return features, encoded_labels
	
	else:
		content_ints = np.asarray([content_ints[i] for i, l in enumerate(content_len) if l>0])
		seq_length = 30
		features = pad_features(content_ints, seq_length=seq_length)
		assert len(features)==len(content_ints), "The features should have as many rows as reviews."
		assert len(features[0])==seq_length, "Each feature row should contain seq_length values."
		return features

def loaders(file, func):	
	if func=='train':
		features, encoded_labels = preprocess(file,func)
		split_frac = 0.80
		## split data into training, validation, and test data (features and labels, x and y)
		split_idx = int(len(features)*split_frac)
		train_x, val_x = features[:split_idx], features[split_idx:]
		train_y, val_y = encoded_labels[:split_idx], encoded_labels[split_idx:]

		# create Tensor datasets
		train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))
		valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))

		# dataloaders
		train_loader = DataLoader(train_data, shuffle=True)#, batch_size=batch_size, drop_last=True)
		valid_loader = DataLoader(valid_data, shuffle=True)#, batch_size=batch_size, drop_last=True
		return train_loader, valid_loader
	else:
		features = preprocess(file, func)
		test_x = features
		test_data = TensorDataset(torch.from_numpy(test_x))
		test_loader = DataLoader(test_data)
		print(test_loader)
		return test_loader

# with open(loc+'label_nums.pkl', 'wb') as f:
# 	pickle.dump(label_dict, f)

"""##MODEL"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class LSTMAttentionModel(nn.Module):
	def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, n_layers, drop_prob=0.5):
		super(LSTMAttentionModel, self).__init__()
		self.batch_size = batch_size
		self.output_size = output_size
		self.hidden_size = hidden_size
		self.vocab_size = vocab_size
		self.embedding_length = embedding_length
		self.n_layers = n_layers

		self.embedding = nn.Embedding(vocab_size, embedding_length)
		self.lstm = nn.LSTM(embedding_length, hidden_size, n_layers, 
							dropout=drop_prob, batch_first = True,)
		self.midl = nn.Linear(hidden_size*3, 150)
		self.drop = nn.Dropout(p=0.3)
		self.label = nn.Linear(150, output_size)
		
	def attention_net(self, lstm_output, final_state):
		hidden = final_state
		hidden = hidden.squeeze(1)
		hidden = torch.t(hidden)
		# print(hidden.squeeze(0).unsqueeze(2).shape)
		# attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)
		# soft_attn_weights = F.softmax(attn_weights, 1)
		# new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)
		attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(0)).squeeze(2)
		soft_attn_weights = F.softmax(attn_weights, 1)
		new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights).squeeze(2)
		new_hidden_state = torch.flatten(new_hidden_state)
		return new_hidden_state.unsqueeze(0)
	
	def forward(self, x, batch_size=1):
		x = x.long()
		embeds = self.embedding(x)
		
		h_0 = Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size).to(device))
		c_0 = Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size).to(device))
			
		output, (final_hidden_state, final_cell_state) = self.lstm(embeds, (h_0, c_0))
		attn_output = self.attention_net(output, final_hidden_state)
		m = self.drop(self.midl(attn_output))
		logits = self.label(m)

		return logits


class RNNAttentionModel(nn.Module):
	def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, n_layers, drop_prob=0.5):
		super(RNNAttentionModel, self).__init__()
		self.batch_size = batch_size
		self.output_size = output_size
		self.hidden_size = hidden_size
		self.vocab_size = vocab_size
		self.embedding_length = embedding_length
		self.n_layers = n_layers

		self.embedding = nn.Embedding(vocab_size, embedding_length)
		self.lstm = nn.RNN(embedding_length, hidden_size, n_layers, 
							dropout=drop_prob, batch_first = True,)
		self.midl = nn.Linear(hidden_size*3, 150)
		self.drop = nn.Dropout(p=0.3)
		self.label = nn.Linear(150, output_size)
		
	def attention_net(self, lstm_output, final_state):
		hidden = final_state
		hidden = hidden.squeeze(1)
		hidden = torch.t(hidden)
		# print(hidden.squeeze(0).unsqueeze(2).shape)
		# attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)
		# soft_attn_weights = F.softmax(attn_weights, 1)
		# new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)

		attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(0)).squeeze(2)
		soft_attn_weights = F.softmax(attn_weights, 1)
		new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights).squeeze(2)
		new_hidden_state = torch.flatten(new_hidden_state)
		return new_hidden_state.unsqueeze(0)
	
	def forward(self, x, batch_size=1):
		x = x.long()
		embeds = self.embedding(x)
		
		h_0 = Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size).to(device))
			
		output, final_hidden_state = self.lstm(embeds, h_0)
		attn_output = self.attention_net(output, final_hidden_state)
		m = self.drop(self.midl(attn_output))
		logits = self.label(m)

		return logits

def choose_model(modeltype, func):	
	if func=='train':
		val = len(vocab_to_int)
	else:
		val = 48591

	vocab_size = val+1 # 48591+1  +1 for the 0 padding + our word tokens
	output_size = 13
	embedding_length = 300
	hidden_size = 256
	n_layers = 3

	if modeltype == 'lstm':
		model = LSTMAttentionModel(vocab_size = vocab_size,
						   output_size = output_size, 
						   embedding_length = embedding_length,
						   hidden_size = hidden_size,
						   n_layers = n_layers,
						   batch_size = 1).to(device)
	else:	
		model = RNNAttentionModel(vocab_size = vocab_size,
						   output_size = output_size, 
						   embedding_length = embedding_length,
						   hidden_size = hidden_size,
						   n_layers = n_layers,
						   batch_size = 1).to(device)
	return model

def savemodel(model, path):
	torch.save(model.state_dict(), path)

def clip_gradient(model, clip_value):
	params = list(filter(lambda p: p.grad is not None, model.parameters()))
	for p in params:
		p.grad.data.clamp_(-clip_value, clip_value)
	
def train_model(model, train_iter, epoch):
	total_epoch_loss = 0
	total_epoch_acc = 0
	lr=0.0001
	loss_fn = F.cross_entropy
	optim = torch.optim.Adam(model.parameters(), lr=lr)
	steps = 0
	idx = -1
	model.train()
	for inputs, labels in train_iter:
		idx += 1
		inputs = inputs.to(device)
		labels = torch.autograd.Variable(labels).long()
		labels = labels.to(device)
		optim.zero_grad()
		prediction = model(inputs)
		loss = loss_fn(prediction, labels)
		num_corrects = (torch.max(prediction, 1)[1].view(labels.size()).data == labels.data).float().sum()
		acc = num_corrects
		loss.backward()
		clip_gradient(model, 1e-1)
		optim.step()
		steps += 1
		
		if steps % 5000 == 0:
			print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')
		
		total_epoch_loss += loss.item()
		total_epoch_acc += acc.item()
	path = 'drive/My Drive/dl_ass2/'+str(epoch+1)
	savemodel(model, path)    
	return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)

def eval_model(model, val_iter):
	total_epoch_loss = 0
	loss_fn = F.cross_entropy
	total_epoch_acc = 0
	model.eval()
	with torch.no_grad():
		for inputs, labels in val_iter:
			inputs = inputs.to(device)
			labels = torch.autograd.Variable(labels).long()
			labels = labels.to(device)
			prediction = model(inputs)
			loss = loss_fn(prediction, labels)
			num_corrects = (torch.max(prediction, 1)[1].view(labels.size()).data == labels.data).sum()
			acc = num_corrects
			total_epoch_loss += loss.item()
			total_epoch_acc += acc.item()
	return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)
	
def trainer(modeltype, file):
	train_loader, valid_loader = loaders(file, 'train')	
	tloss = []
	vloss = []
	tacc = []
	vacc = []
	model = choose_model(modeltype, 'train')
	for epoch in range(10):
		train_loss, train_acc = train_model(model, train_loader, epoch)
		val_loss, val_acc = eval_model(model, valid_loader)
		tloss.append(train_loss)
		vloss.append(val_loss)
		tacc.append(train_acc)
		vacc.append(val_acc)
		print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')

	with open(loc+'train_acc.pkl', 'wb') as f:
		pickle.dump(tacc, f)
	with open(loc+'val_acc.pkl', 'wb') as f:
		pickle.dump(vacc, f)
	with open(loc+'train_loss.pkl', 'wb') as f:
		pickle.dump(tloss, f)
	with open(loc+'val_loss.pkl', 'wb') as f:
		pickle.dump(vloss, f)

l1 = loc + 'emotion.csv'
emo = pd.read_csv(l1)
emo.drop(emo.columns[[0, 2]], axis = 1, inplace = True)
dat = emo.head(35000)
dat = emo.tail(10000)

#******************************************************************************************************************
def plots(item_train, item_valid, title):
	'''
	PLotting the loss and accuracy plots
	'''
	plt.figure()
	plt.plot(item_train, label='training')
	plt.plot(item_valid, label='validation')
	plt.title(title)
	plt.legend(loc='best')
	plt.savefig(loc+title+'.png')

def plotter():	
	'''
	Helper function for plotting the 
	1. loss plots
	2. accuracy plots
	'''
	with open(loc+'train_acc.pkl', 'rb') as f:
		train_acc = pickle.load(f)
	with open(loc+'val_acc.pkl', 'rb') as f:
		val_acc = pickle.load(f)
	with open(loc+'train_loss.pkl', 'rb') as f:
		train_loss = pickle.load(f)
	with open(loc+'val_loss.pkl', 'rb') as f:
		val_loss = pickle.load(f)

	plts = [train_acc, val_acc, train_loss, val_loss]
	title = ['accuracy RNN', 'loss RNN']
	plots(plts[0], plts[1], title[0])
	plots(plts[2], plts[3], title[1])

def plot_confusion(array, title):       
	'''
	PLotting the confusion matrix
	'''
	df_cm = pd.DataFrame(array, range(13), range(13))
	plt.figure(figsize=(20,20))
	sn.heatmap(df_cm, annot=True, annot_kws={"size": 12})
	plt.savefig(loc+title+'.png')
	plt.show()

def plot_roc(y_test, y_pred,name):
	'''
	PLotting the roc curves for the predictions
	'''
	y_test = []
	y_pred = []
	for t, p in zip(labs,preds):
		y_test.append(t[0])
		y_pred.append(p[0])
	classes = list(set(y_test))
	print(len(classes))
	print(metrics.accuracy_score(y_test, y_pred))
	y_test = label_binarize(y_test,classes)
	y_pred = label_binarize(y_pred,classes)
	plt.figure()
	fpr = dict()
	tpr = dict()
	roc_auc = dict()
	n_classes = len(classes)

	mean_fpr = np.linspace(0,1,100)
	tprs = []
	aucs = []
	for i in range(n_classes):
		fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])
		roc_auc[i] = auc(fpr[i], tpr[i])
		aucs.append(roc_auc[i])
		tprs.append(interp(mean_fpr, fpr[i], tpr[i]))
		tprs[-1][0] = 0.0
		plt.plot(fpr[i], tpr[i], lw=1, alpha=0.3)

	mean_tpr = np.mean(tprs, axis=0)
	mean_tpr[-1] = 1.0
	mean_auc = auc(mean_fpr, mean_tpr)
	std_auc = np.std(aucs)
	plt.plot(mean_fpr, mean_tpr, color='b',
			label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
			lw=2, alpha=.8)

	std_tpr = np.std(tprs, axis=0)
	tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
	tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
	plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,
					label=r'$\pm$ 1 std. dev.')

	plt.plot([0, 1], [0, 1], color='navy', alpha=0.8, lw=2, linestyle='--')
	plt.xlim([0.0, 1.0])
	plt.ylim([0.0, 1.05])
	plt.xlabel('False Positive Rate')
	plt.ylabel('True Positive Rate')
	plt.title(name)
	plt.legend(loc="lower right")
	plt.savefig(loc+name+'roc'+'.png')
 
def create_confusion(model, data_iter,name):    
	'''
	Helper function for creating
	1. Confusion Matrix
	2. ROC plots
	'''
	preds = []
	labs = []
	total_epoch_acc = 0
	model.eval()
	with torch.no_grad():
		for inputs, labels in data_iter:
			inputs = inputs.to(device)
			labels = torch.autograd.Variable(labels).long()
			labels = labels.to(device)
			prediction = model(inputs)
			labs.append(labels.data.detach().cpu().clone().numpy())
			# print(type(labs[0]))
			# break
			num_corrects = (torch.max(prediction, 1)[1].view(labels.size()).data == labels.data).sum()
			acc = num_corrects
			total_epoch_acc += acc.item()
			preds.append(torch.max(prediction, 1)[1].view(labels.size()).data.detach().cpu().clone().numpy())
	cm = confusion_matrix(labs, preds)
	print(total_epoch_acc/len(data_iter))
	plot_roc(labs, preds, name)
	plot_confusion(cm,name)

#******************************************************************************************************************

def predictions(modeltype, file, modelpath, name):
	model = choose_model(modeltype, 'test')
	data_iter = loaders(file, 'test')
	preds = []
	model.load_state_dict(torch.load(loc+modelpath))
	model.eval()
	with torch.no_grad():
		for inputs in data_iter:
			# print(inputs)
			inputs = inputs[0].to(device)
			# labels = torch.autograd.Variable(labels).long()
			# labels = labels.to(device)
			prediction = model(inputs)
			# num_corrects = (torch.max(prediction, 1)[1].view(labels.size()).data == labels.data).sum()
			# acc = num_corrects
			# total_epoch_acc += acc.item()
			preds.append(torch.max(prediction, 1)[1].view(1).data.detach().cpu().clone().numpy())
	# print(total_epoch_acc/len(data_iter))
	reverse_label = {}
	label_dict = {'anger': 12,'boredom': 10,'empty': 0,'enthusiasm': 2,
		'fun': 7,'happiness': 9,'hate': 8,'love': 6,'neutral': 3,
		'relief': 11,'sadness': 1,'surprise': 5,'worry': 4}
	for i in range(13):
		for key in label_dict:
			if label_dict[key] == i:
				reverse_label[i] = key
	ID = []
	sentiment = []
	y_pred = []
	for p in preds:
		y_pred.append(p[0])
	for i, lab in enumerate(y_pred):
		ID.append(i+1)
		sentiment.append(reverse_label[lab])
	df = pd.DataFrame(list(zip(ID, sentiment)), 
				columns =['ID', 'Class'])
	df.to_csv(loc+name+'.csv', encoding='utf-8', index=False)

if __name__=="__main__":
	# train_file = 'train_data.csv'
	modeltype = 'rnn' #rnn/lstm
	modelpath = 'rnn/4' #lstm/1
	# trainer(modeltype, train_file)
	test_file = 'test.csv'
	predictions(modeltype, test_file, modelpath, 'results')
 	